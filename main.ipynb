{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wbdata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "import re\n",
    "from ast import literal_eval  #Is used to safely evaluate a string that looks like a Python literal \n",
    "import logging\n",
    "logging.getLogger('shelved_cache.persistent_cache').setLevel(logging.ERROR) #Libraries warnings\n",
    "warnings.filterwarnings('ignore') #Python warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Use OpenAI tokenizer to calculate token lengths (optional, more accurate)\n",
    "def estimate_token_count(text, model=\"gpt-3.5-turbo-instruct\"):\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    return len(enc.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_prompt  = \"\"\"\n",
    "You are a data scientist tasked with selecting the most relevant development indicators to predict when a country will reach the GDP per capita level of developed countries.\n",
    "\n",
    "Below is a list of available indicators retrieved from the World Bank data source 2. Each item consists of an indicator code and its corresponding name:\n",
    "\n",
    "{chunked_indicators}\n",
    "\n",
    "Your task:\n",
    "- Select indicators that are **strongly related to GDP growth**, **economic development**, or **structural transition** (e.g. education, infrastructure, industrialization, trade, institutions).\n",
    "- Prefer indicators that are **leading indicators** or have **causal influence** on development, not just GDP itself.\n",
    "- Include **diverse dimensions**: macroeconomic, infrastructure, education, health, technology, labor, governance, etc.\n",
    "- Limit to {number_of_indicators_per_chunk} indicators.\n",
    "- Output only a Python dictionary like this without any comments:\n",
    "\n",
    "dict_indicators = {{\n",
    "    'CODE1': 'Indicator Name 1',\n",
    "    'CODE2': 'Indicator Name 2',\n",
    "    ...\n",
    "}} \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "available_indicators = wbdata.get_indicators(source=2)\n",
    "# available_str = \"\\n\".join([f\"{i['id']}: {i['name']}\" for i in available_indicators])\n",
    "df_indicators = pd.DataFrame(available_indicators)[['id','name']]\n",
    "\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo-instruct\")\n",
    "df_indicators[\"token_count\"] = df_indicators[\"name\"].apply(lambda x: len(enc.encode(x)))\n",
    "\n",
    "base_prompt_count = estimate_token_count(base_prompt)\n",
    "\n",
    "\n",
    "\n",
    "model_max_tokens = 2500   # Model's maximum context length is 4097 tokens\n",
    "\n",
    "max_prompt_tokens = model_max_tokens - base_prompt_count\n",
    "\n",
    "chunk_index = []\n",
    "\n",
    "for i in range(1,len(df_indicators)):\n",
    "    if len(chunk_index) == 0:\n",
    "        list_indicators = \"\\n\".join((df_indicators[\"id\"][0:i] + \": \" + df_indicators[\"name\"][0:i]).tolist())\n",
    "        token_count = estimate_token_count(list_indicators)\n",
    "    else:\n",
    "        list_indicators = \"\\n\".join((df_indicators[\"id\"][current_index:i] + \": \" + df_indicators[\"name\"][current_index:i]).tolist())\n",
    "        token_count = estimate_token_count(list_indicators)\n",
    "    if token_count>max_prompt_tokens:\n",
    "        current_index = i\n",
    "        chunk_index.append(i)\n",
    "        token_count = 0\n",
    "\n",
    "list_indicators = (df_indicators[\"id\"] + \": \" + df_indicators[\"name\"]).tolist()\n",
    "\n",
    "chunks = [\n",
    "    list_indicators[0:index] if i == 0 \n",
    "    else list_indicators[chunk_index[i-1]:index]\n",
    "    for i, index in enumerate(chunk_index)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "with open(\"credentials.json\", \"r\") as file:\n",
    "    credentials = json.load(file)\n",
    "\n",
    "# Initialize the client\n",
    "client = OpenAI(\n",
    "    api_key=credentials[\"OPENAI_API_KEY\"],\n",
    "    base_url=credentials[\"OPENAI_API_BASE\"]  # Only if using a proxy/alternative endpoint\n",
    ")\n",
    "\n",
    "# Decoding parameters\n",
    "TEMPERATURE = 0.0\n",
    "def query_openai(prompt):\n",
    "\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=300  \n",
    "\n",
    "    )\n",
    "    time.sleep(5)  # rate limiting\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running chunk 1/14...\n",
      "Attempt 1... Success.\n",
      "\n",
      " Running chunk 2/14...\n",
      "Attempt 1... Success.\n",
      "\n",
      " Running chunk 3/14...\n",
      "Attempt 1... Success.\n",
      "\n",
      " Running chunk 4/14...\n",
      "Attempt 1... Success.\n",
      "\n",
      " Running chunk 5/14...\n",
      "Attempt 1... Success.\n",
      "\n",
      " Running chunk 6/14...\n",
      "Attempt 1... Success.\n",
      "\n",
      " Running chunk 7/14...\n",
      "Attempt 1... Success.\n",
      "\n",
      " Running chunk 8/14...\n",
      "Attempt 1... Success.\n",
      "\n",
      " Running chunk 9/14...\n",
      "Attempt 1... \n",
      " Running chunk 10/14...\n",
      "Attempt 1... Success.\n",
      "\n",
      " Running chunk 11/14...\n",
      "Attempt 1... Success.\n",
      "\n",
      " Running chunk 12/14...\n",
      "Attempt 1... Success.\n",
      "\n",
      " Running chunk 13/14...\n",
      "Attempt 1... Success.\n",
      "\n",
      " Running chunk 14/14...\n",
      "Attempt 1... Success.\n"
     ]
    }
   ],
   "source": [
    "max_indicators = 30\n",
    "n_chunks =  len(chunks)\n",
    "number_of_indicators_per_chunk = math.ceil(max_indicators/n_chunks)\n",
    "\n",
    "merged_dict = {}\n",
    "\n",
    "\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    print(f\"\\n Running chunk {idx+1}/{n_chunks}...\")\n",
    "    chunk_text = \"\\n\".join(chunk)\n",
    "\n",
    "    prompt = base_prompt.format(\n",
    "        chunked_indicators=chunk_text,\n",
    "        number_of_indicators_per_chunk=number_of_indicators_per_chunk\n",
    "    )\n",
    "    \n",
    "    success = False\n",
    "    retries = 3\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt}...\", end=\" \")\n",
    "            response = query_openai(prompt)\n",
    "            \n",
    "            # extract dict with regex\n",
    "            raw_text = response.choices[0].text.replace('\\r', '').replace('\\n', '').replace('\\t', '').strip()\n",
    "\n",
    "            match = re.search(r\"\\{.*?\\}\", raw_text, re.DOTALL)\n",
    "            if not match:\n",
    "                raise ValueError(\"No dictionary found in model response.\")\n",
    "            \n",
    "            chunk_dict = literal_eval(match.group(0))\n",
    "            merged_dict.update(chunk_dict)\n",
    "            print(\"Success.\")\n",
    "            success = True\n",
    "            break\n",
    "        except Exception as e:\n",
    "\n",
    "            print(f\"Error: {e}\")\n",
    "            if attempt < retries:\n",
    "                time.sleep(5)  # wait before retry\n",
    "            else:\n",
    "                print(f\"Failed chunk {idx+1} after {retries} attempts.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Completion(id=None, choices=None, created=None, model=None, object=None, system_fingerprint=None, usage=None, error={'message': 'Too many parallel completions requested. You submitted 104 prompts, but you can currently request up to at most a total of 20). Please contact us through our help center at help.openai.com for further questions.', 'type': 'invalid_request_error', 'param': None, 'code': None})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Completion(id=None, choices=None, created=None, model=None, object=None, system_fingerprint=None, usage=None, error={'message': 'Too many parallel completions requested. You submitted 104 prompts, but you can currently request up to at most a total of 20). Please contact us through our help center at help.openai.com for further questions.', 'type': 'invalid_request_error', 'param': None, 'code': None})"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "america_countries = [\n",
    "    'ARG', 'BLZ', 'BOL', 'BRA', 'CAN', 'CHL', 'COL', \n",
    "    'CRI', 'ECU', 'SLV', 'GTM', 'GUY', 'HND', 'MEX', \n",
    "    'NIC', 'PAN', 'PRY', 'PER', 'SUR', 'USA', 'URY', 'VEN'\n",
    "]\n",
    "\n",
    "df = wbdata.get_dataframe(merged_dict, country=america_countries, parse_dates=True,date=(\"2000\", \"2025\"))\n",
    "df.reset_index(inplace = True)\n",
    "# df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate null rate by country\n",
    "null_rates = df.groupby('country').apply(lambda x: x.isnull().mean())\n",
    "\n",
    "# The country is already the index, so we don't need to reset it\n",
    "# Just transpose the DataFrame for better visualization\n",
    "null_rates = null_rates.T  # This puts countries as columns and variables as rows\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.heatmap(\n",
    "    null_rates,\n",
    "    cmap='YlOrRd',\n",
    "    annot=True,\n",
    "    fmt='.1%',\n",
    "    linewidths=.5,\n",
    "    cbar_kws={'label': 'Null Rate'}\n",
    ")\n",
    "\n",
    "plt.title('Null Rate by Country and Column')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Variable')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is possible to see that the international migrant stock (% of popolation) isnt a god indicator, since most of its values are null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate null rate by country\n",
    "null_rates = df.groupby('date').apply(lambda x: x.isnull().mean())\n",
    "\n",
    "# The country is already the index, so we don't need to reset it\n",
    "# Just transpose the DataFrame for better visualization\n",
    "null_rates = null_rates.T  # This puts countries as columns and variables as rows\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.heatmap(\n",
    "    null_rates,\n",
    "    cmap='YlOrRd',\n",
    "    annot=True,\n",
    "    fmt='.1%',\n",
    "    linewidths=.5,\n",
    "    cbar_kws={'label': 'Null Rate'}\n",
    ")\n",
    "\n",
    "plt.title('Null Rate by Date')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Variable')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this heat maps, we can see that the international migran stock indicator is not a good  indicator for us, since in many years we dont have any values in it, so we need to change that, furthermore we can remove that 2024 year from our data ser, for the same reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.4  # % of missingness\n",
    "\n",
    "null_rates_date_country = df.groupby(['country']).apply(lambda x: x.isnull().mean()).drop(columns = ['date','country'] )\n",
    "column_null_rates = null_rates_date_country.mean()\n",
    "columns_to_keep = column_null_rates[column_null_rates < threshold].index\n",
    "df_filtered = null_rates_date_country[columns_to_keep].reset_index()\n",
    "df_filtered = df_filtered[df.date < '2024-01-01'] # to remove 2024 from the data set\n",
    "\n",
    "print(f'Columns that have been dropped {list(column_null_rates[column_null_rates > threshold].index)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets replace that with new indicators, to furfill our goal to have at leat two indicators for each indicator group. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
